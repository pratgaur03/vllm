INFO 07-09 06:39:51 [__init__.py:239] Automatically detected platform rocm.
Namespace(backend='openai-chat', base_url=None, host='localhost', port=8080, endpoint='/v1/chat/completions', dataset_name='random', dataset_path=None, max_concurrency=None, model='/workspace/llama_3.1_70B', tokenizer=None, use_beam_search=False, num_prompts=1024, logprobs=None, request_rate=50.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=True, save_detailed=False, metadata=None, result_dir='./results', result_filename='chunked_prefill-qps-50-input-len-8192-tp-8.json', ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=8192, random_output_len=256, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)
Starting initial single prompt test run...
=== DEBUG: Entering benchmark() ===
DEBUG: backend         = openai-chat
DEBUG: api_url         = 'http://localhost:8080/v1/chat/completions'
DEBUG: model_id        = '/workspace/llama_3.1_70B'
DEBUG: model_name      = None
DEBUG: request_func    = async_request_openai_chat_completions
Starting initial single prompt test run...
test_output RequestFuncOutput(generated_text="It seems like you've provided a large amount of text that appears to be a mix of programming code, technical terms, and random words. I'll do my best to help you make sense of it.\n\nFrom what I can gather, this text seems to be a collection of snippets from various programming languages, including Java, Python, and C++. There are also references to technical terms like machine learning, data structures, and algorithms.\n\nHowever, the text also contains a lot of random words and phrases that don't seem to be related to programming or technical topics. It's possible that this text was generated by a machine learning model or a random text generator.\n\nIf you could provide more context about where this text came from and what you're trying to achieve, I'd be happy to help you make sense of it. Are you trying to:\n\n1. Understand a specific programming concept or technique?\n2. Debug a piece of code?\n3. Generate text using a machine learning model?\n4. Something else?\n\nPlease let me know, and I'll do my best to assist you.", success=True, latency=9.568054017145187, output_tokens=217, ttft=0.44383111828938127, itl=[1.0732780057005584, 0.03671560296788812, 0.03771384200081229, 0.0374396201223135, 0.03721406310796738, 0.03772568888962269, 0.03759755287319422, 0.03709540329873562, 0.03769897297024727, 0.03723766002804041, 0.03736990876495838, 0.03751587029546499, 0.037015297915786505, 0.03717029793187976, 0.0371261858381331, 0.037021880969405174, 0.037121625151485205, 0.037088009994477034, 0.03711059922352433, 0.03720862464979291, 0.037268792279064655, 0.03694381611421704, 0.037574846763163805, 0.03698086505755782, 0.03778633987531066, 0.037340905983000994, 0.03737706318497658, 0.03748709801584482, 0.03743186593055725, 0.03702930174767971, 0.03706562612205744, 0.037123851012438536, 0.03737619426101446, 0.0369941140525043, 0.03721469268202782, 0.037120826076716185, 0.037340675946325064, 0.037025302182883024, 0.037258986849337816, 0.03712919307872653, 0.03715160582214594, 0.03723005624487996, 0.03743287594988942, 0.03756509302183986, 0.03722896100953221, 0.0376107357442379, 0.03745912527665496, 0.03699219273403287, 0.037105624098330736, 0.037055949214845896, 0.03700598096475005, 0.03704264806583524, 0.037077900022268295, 0.03710006969049573, 0.03736558323726058, 0.036981815937906504, 0.037336213048547506, 0.0371645400300622, 0.03704956779256463, 0.03719544783234596, 0.03733112337067723, 0.03668448608368635, 0.037450645584613085, 0.03668580297380686, 0.03701960202306509, 0.03727722633630037, 0.03709190106019378, 0.03725889790803194, 0.03696776367723942, 0.03680194728076458, 0.03739641606807709, 0.03707558196038008, 0.0370265981182456, 0.037447614595294, 0.037017472088336945, 0.03748523211106658, 0.03699543606489897, 0.037026410922408104, 0.03709843987599015, 0.03697159932926297, 0.036984705831855536, 0.03718581097200513, 0.03714238479733467, 0.03711083810776472, 0.03729352308437228, 0.03732365882024169, 0.03696343628689647, 0.03743996378034353, 0.037001521326601505, 0.03754986356943846, 0.03703393926844001, 0.03698290791362524, 0.03698385786265135, 0.03708316711708903, 0.037065450102090836, 0.037093725986778736, 0.037103149108588696, 0.03710960503667593, 0.03709637559950352, 0.03724250430241227, 0.0374976871535182, 0.03686083294451237, 0.0371799529530108, 0.037106540985405445, 0.03716583410277963, 0.03770862985402346, 0.03753880085423589, 0.03710920317098498, 0.037408275064080954, 0.03719044104218483, 0.03761532390490174, 0.03714034194126725, 0.03708820603787899, 0.037796752993017435, 0.03698115376755595, 0.03746479703113437, 0.036853799130767584, 0.03717359621077776, 0.03755528386682272, 0.03711039898917079, 0.0372965456917882, 0.037152080330997705, 0.03701813705265522, 0.037572947796434164, 0.037037411238998175, 0.037019474897533655, 0.037122849840670824, 0.03748931409791112, 0.03699188819155097, 0.037265000864863396, 0.03669001813977957, 0.03732176870107651, 0.037205581087619066, 0.03673092508688569, 0.03717889171093702, 0.037164345383644104, 0.036882593762129545, 0.03738617990165949, 0.03660827223211527, 0.03718670178204775, 0.036916034296154976, 0.03704091487452388, 0.03713086200878024, 0.03701672703027725, 0.03733394015580416, 0.037558003794401884, 0.041481811087578535, 0.041289322078228, 0.041665283031761646, 0.04025356378406286, 0.03714106883853674, 0.03724898537620902, 0.03711853455752134, 0.037615242414176464, 0.036862798035144806, 0.03701870562508702, 0.03697836818173528, 0.03726040106266737, 0.036985390819609165, 0.03802081290632486, 0.03696944937109947, 0.037202073726803064, 0.03721958491951227, 0.036883185151964426, 0.03728979919105768, 0.03708974597975612, 0.0370989846996963, 0.03735526418313384, 0.03710005385801196, 0.037106048315763474, 0.03718803869560361, 0.03753249114379287, 0.03715211898088455, 0.03708337480202317, 0.03709719609469175, 0.03720658412203193, 0.03715291293337941, 0.03713852213695645, 0.03711679670959711, 0.037071706261485815, 0.03744016867130995, 0.037297158036381006, 0.03708431497216225, 0.03711686609312892, 0.03705650707706809, 0.037029465194791555, 0.03697458375245333, 0.037160097155719995, 0.03700122702866793, 0.037056068889796734, 0.03704432025551796, 0.03729868680238724, 0.03755895886570215, 0.03761471016332507, 0.037645280826836824, 0.03741913428530097, 0.03715258790180087, 0.03734640311449766, 0.03695631492882967, 0.03730717767030001, 0.03721041325479746, 0.037213439121842384, 0.0372609980404377, 0.03734441986307502, 0.03746864711865783, 0.03741985559463501, 0.0369543069973588, 0.0373259112238884, 0.03716627601534128, 0.03706382680684328, 0.03737018117681146, 0.037020054180175066, 0.03718901891261339, 0.03675373084843159, 0.036969796288758516, 0.03736829897388816, 0.038304568734019995], tpot=0.0, prompt_len=8192, error='')
Initial test run completed. Starting main benchmark run...
Traffic request rate: 50.0
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: None
============ Serving Benchmark Result ============
Successful requests:                     377       
Benchmark duration (s):                  21622.80  
Total input tokens:                      3088384   
Total generated tokens:                  83773     
Request throughput (req/s):              0.02      
Output token throughput (tok/s):         3.87      
Total Token throughput (tok/s):          146.70    
---------------Time to First Token----------------
Mean TTFT (ms):                          117467.85 
Median TTFT (ms):                        116664.17 
P99 TTFT (ms):                           258124.28 
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          309.75    
Median TPOT (ms):                        305.44    
P99 TPOT (ms):                           395.67    
---------------Inter-token Latency----------------
Mean ITL (ms):                           304.21    
Median ITL (ms):                         279.29    
P99 ITL (ms):                            465.80    
==================================================
